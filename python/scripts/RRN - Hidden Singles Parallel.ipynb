{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = '/home/ajhnam/projects/hidden_singles_public/'\n",
    "save_path = '/data2/pdp/ajhnam/hidden_singles_public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(proj_path + 'python/')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "from hiddensingles.misc import torch_utils as tu\n",
    "from hiddensingles.misc import pd_utils as pu\n",
    "from hiddensingles.misc import utils, TensorDict, RRN, DigitRRN, GPUMultiprocessor\n",
    "from hiddensingles.experiment.sudoku_hs_service import create_tutorial, create_phase1, create_phase2\n",
    "\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, dataset, num_steps=8):\n",
    "    outputs = model(dataset.inputs, num_steps=num_steps)\n",
    "    \n",
    "    goals = tu.expand_along_dim(dataset.goals, 1, num_steps)\n",
    "    goal_outputs = tu.select(outputs, goals, select_dims=1)\n",
    "    \n",
    "    targets = tu.expand_along_dim(dataset.targets, 1, num_steps)\n",
    "    goal_loss = tu.cross_entropy(goal_outputs, targets)\n",
    "    goal_probs = tu.select(goal_outputs.softmax(-1), dataset.targets)\n",
    "    goal_td = TensorDict(loss=goal_loss,\n",
    "                         probs=goal_probs,\n",
    "                         outputs=goal_outputs)\n",
    "    \n",
    "    coords = tu.expand_along_dim(dataset.coords, 1, num_steps)\n",
    "    out_exp = tu.expand_along_dim(outputs, 2, 9)\n",
    "    coord_outputs = tu.select(out_exp, coords, select_dims=1)\n",
    "    coord_targets = tu.expand_along_dim(dataset.coord_targets, 1, num_steps)\n",
    "    coord_loss = tu.cross_entropy(coord_outputs, coord_targets)\n",
    "    coord_probs = tu.select_subtensors(coord_outputs.softmax(-1), coord_targets)\n",
    "    coord_td = TensorDict(loss=coord_loss,\n",
    "                          probs=coord_probs,\n",
    "                          outputs=coord_outputs)\n",
    "\n",
    "    loss = goal_loss + coord_loss\n",
    "    correct = dataset.targets == goal_outputs[:,-1].argmax(-1)\n",
    "    preds = Categorical(goal_outputs[:,-1].softmax(-1)).sample()\n",
    "    sample_correct = dataset.targets == preds\n",
    "    return TensorDict(loss=loss,\n",
    "                      correct=correct,\n",
    "                      sample_correct = sample_correct,\n",
    "                      outputs=outputs,\n",
    "                      goal=goal_td,\n",
    "                      coord=coord_td)\n",
    "\n",
    "def get_accuracy(model, dataset, num_steps):\n",
    "    with torch.no_grad():\n",
    "        results = get_results(model, dataset, num_steps=num_steps)\n",
    "    accuracy = results.correct.float().mean().item()\n",
    "    sample_accuracy = results.sample_correct.float().mean().item()\n",
    "    return accuracy, sample_accuracy\n",
    "\n",
    "def get_test_results(model, test_dset, df_conditions, num_steps):\n",
    "    with torch.no_grad():\n",
    "        test_results = get_results(model, test_dset, num_steps=num_steps)\n",
    "\n",
    "    df = test_results[['correct', 'sample_correct']].to_dataframe({0: 'puzzle_id'})\n",
    "    df = df.merge(df_conditions, on=['puzzle_id'])\n",
    "    return df.drop('run_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase2_conditions(phase2):\n",
    "    ht = [p.condition.house_type for p in phase2]\n",
    "    hi = [p.condition.house_index for p in phase2]\n",
    "    ci = [p.condition.cell_index for p in phase2]\n",
    "    ds = [p.condition.digit_set for p in phase2]\n",
    "    conditions = pd.DataFrame(np.array([ht, hi, ci, ds]).T,\n",
    "                              columns=['house_type', 'house_index', 'cell_index', 'digit_set'])\n",
    "    return conditions\n",
    "\n",
    "def hidden_singles_to_tensordict(list_of_hidden_singles, digit_rrn, device='cpu'):\n",
    "    grids = torch.tensor([a.grid.array for a in list_of_hidden_singles], device=device)\n",
    "    goals = [p.coordinates['goal'] for p in list_of_hidden_singles]\n",
    "    goals = torch.tensor([[g.x, g.y] for g in goals], device=device)\n",
    "    targets = torch.tensor([a.digits['target'] for a in list_of_hidden_singles], device=device) - 1 # make it 0-8\n",
    "    coords = grids.nonzero()[:,1:].view(len(list_of_hidden_singles), -1, 2)\n",
    "    coord_targets = tu.select(tu.expand_along_dim(grids, 1, 9), coords) - 1 # make it 0-8\n",
    "    \n",
    "    inputs = DigitRRN.make_onehot(grids) if digit_rrn else grids\n",
    "    return TensorDict(inputs=inputs,\n",
    "                      grids=grids,\n",
    "                      goals=goals,\n",
    "                      targets=targets,\n",
    "                      coords=coords,\n",
    "                      coord_targets=coord_targets)\n",
    "\n",
    "def create_dataset(num_phase1):\n",
    "    digit_set1 = set(random.sample(set(range(1, 10)), 4))\n",
    "    digit_set2 = set(random.sample(set(range(1, 10)) - digit_set1, 4))\n",
    "    tutorial = create_tutorial(digit_set1)\n",
    "    phase1 = create_phase1(tutorial, num_phase1)\n",
    "    phase2 = create_phase2(tutorial, digit_set1, digit_set2)\n",
    "    conditions = get_phase2_conditions(phase2)\n",
    "    return phase1, phase2, conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_id, num_train, model_type, device):\n",
    "    assert model_type in ('rrn', 'drrn')\n",
    "    time_start = datetime.now()\n",
    "    print(\"{} Starting run: {}, model: {}, num_train: {}, device: {}\".format(\n",
    "        time_start.strftime(\"%Y-%m-%d %H:%M:%S\"), run_id, model_type, num_train, device), end='\\n')\n",
    "    dirpath = save_path + '{}_hs/tr{}_rid{}/'.format(model_type, num_train, run_id)\n",
    "    utils.mkdir(dirpath)\n",
    "    \n",
    "#     num_puzzle_presentations = 1000\n",
    "    num_puzzle_presentations = 5000000\n",
    "    num_steps = 8\n",
    "    if model_type == 'rrn':\n",
    "        model = RRN(digit_embed_size=10,\n",
    "                    num_mlp_layers=0,\n",
    "                    hidden_vector_size=48,\n",
    "                    message_size=48,\n",
    "                    encode_coordinates=False).to(device)\n",
    "        batch_size = 100\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        model = DigitRRN(hidden_vector_size=16,\n",
    "                         message_size=16).to(device)\n",
    "        batch_size = 10\n",
    "        learning_rate = 1e-4\n",
    "    num_grad_updates = num_puzzle_presentations // batch_size\n",
    "    \n",
    "    dataset = TensorDict.load(save_path + 'hs_data.td')\n",
    "    dataset = dataset[model_type][run_id].to(device)\n",
    "    ds_train = dataset.train[:num_train].repeat(0, 500//num_train)\n",
    "    dataloader = DataLoader(ds_train.to_dataset(),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "    df_conditions = pd.read_csv(save_path + 'hs_conditions.tsv', sep='\\t')\n",
    "    df_conditions = df_conditions[df_conditions.run_id == run_id]\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    num_updates = 0\n",
    "    done = False\n",
    "    acc_v = 0\n",
    "    \n",
    "    all_tr_results = []\n",
    "    all_te_results = []\n",
    "    acc_tr, sample_acc_tr = get_accuracy(model, dataset.train[:num_train], num_steps=8)\n",
    "    acc_v, sample_acc_v = get_accuracy(model, dataset.valid, num_steps=8)\n",
    "    all_tr_results.append({\n",
    "        'model_type': model_type,\n",
    "        'run_id': run_id,\n",
    "        'num_train': num_train,\n",
    "        'batch_size': batch_size,\n",
    "        'num_updates': num_updates,\n",
    "        'acc_top_train': acc_tr,\n",
    "        'acc_sample_train': sample_acc_tr,\n",
    "        'acc_top_valid': acc_v,\n",
    "        'acc_sample_valid': sample_acc_v\n",
    "    })\n",
    "    te_results = get_test_results(model, dataset.test, df_conditions, num_steps=8)\n",
    "    te_results['model_type'] = model_type\n",
    "    te_results['run_id'] = run_id\n",
    "    te_results['num_train'] = num_train\n",
    "    te_results['num_updates'] = 0\n",
    "    all_te_results.append(te_results)\n",
    "    \n",
    "    while not done:\n",
    "        for dset in dataloader:\n",
    "            dset = TensorDict(**dset)\n",
    "            optimizer.zero_grad()\n",
    "            results = get_results(model, dset, num_steps=num_steps)\n",
    "            results.loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            num_updates += 1\n",
    "            if num_updates % 10 == 0:\n",
    "                acc_tr, sample_acc_tr = get_accuracy(model, dataset.train[:num_train], num_steps=8)\n",
    "                acc_v, sample_acc_v = get_accuracy(model, dataset.valid, num_steps=8)\n",
    "                all_tr_results.append({\n",
    "                    'model_type': model_type,\n",
    "                    'run_id': run_id,\n",
    "                    'num_train': num_train,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_updates': num_updates,\n",
    "                    'acc_top_train': acc_tr,\n",
    "                    'acc_sample_train': sample_acc_tr,\n",
    "                    'acc_top_valid': acc_v,\n",
    "                    'acc_sample_valid': sample_acc_v\n",
    "                })\n",
    "                te_results = get_test_results(model, dataset.test, df_conditions, num_steps=8)\n",
    "                te_results['model_type'] = model_type\n",
    "                te_results['run_id'] = run_id\n",
    "                te_results['num_train'] = num_train\n",
    "                te_results['num_updates'] = num_updates\n",
    "                all_te_results.append(te_results)\n",
    "                \n",
    "            if acc_v >= .99 or num_updates >= num_grad_updates:\n",
    "                done = True\n",
    "                break\n",
    "                \n",
    "            if num_updates % 10000 == 0:\n",
    "                pd.DataFrame(all_tr_results).to_csv(dirpath + 'tr_results.tsv', sep='\\t', index=False)\n",
    "                all_te_results = [pd.concat(all_te_results)]\n",
    "                all_te_results[0].to_csv(dirpath + 'te_results.tsv', sep='\\t', index=False)\n",
    "                torch.save(model.state_dict(), os.path.join(dirpath, '{}.mdl'.format(num_updates)))\n",
    "                \n",
    "    pd.DataFrame(all_tr_results).to_csv(dirpath + 'tr_results.tsv', sep='\\t', index=False)\n",
    "    all_te_results = [pd.concat(all_te_results)]\n",
    "    all_te_results[0].to_csv(dirpath + 'te_results.tsv', sep='\\t', index=False)\n",
    "    torch.save(model.state_dict(), os.path.join(dirpath, '{}.mdl'.format(num_updates)))\n",
    "            \n",
    "    time_end = datetime.now()\n",
    "    elapsed = str(time_end - time_start)\n",
    "    print(\"{} Completed in {}. run: {}, model: {}, num_train: {}, num_updates: {}, device: {}\".format(\n",
    "        time_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        str(elapsed),\n",
    "        run_id, model_type, num_train, num_updates, device), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models\n",
    "\n",
    "Creating dataset and training models should happen only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "num_train = 500\n",
    "num_valid = 100\n",
    "load = True\n",
    "\n",
    "if load:\n",
    "    dataset = TensorDict.load(save_path + 'hs_data.td')\n",
    "    df_conditions = pd.read_csv(save_path + 'hs_conditions.tsv', sep='\\t')\n",
    "else:\n",
    "    datasets = []\n",
    "    all_conditions = []\n",
    "    for i in range(num_runs):\n",
    "        phase1, phase2, conditions = create_dataset(num_phase1=num_train+num_valid)\n",
    "        conditions['run_id'] = i\n",
    "        conditions['puzzle_id'] = range(64)\n",
    "\n",
    "        rrn_phase1 = hidden_singles_to_tensordict(phase1, digit_rrn=False)\n",
    "        rrn_phase2 = hidden_singles_to_tensordict([p.hidden_single for p in phase2], digit_rrn=False)\n",
    "        drrn_phase1 = hidden_singles_to_tensordict(phase1, digit_rrn=True)\n",
    "        drrn_phase2 = hidden_singles_to_tensordict([p.hidden_single for p in phase2], digit_rrn=True)\n",
    "        rrn_dset = TensorDict(train=rrn_phase1[:num_train],\n",
    "                              valid=rrn_phase1[num_train:],\n",
    "                              test=rrn_phase2)\n",
    "        drrn_dset = TensorDict(train=drrn_phase1[:num_train],\n",
    "                               valid=drrn_phase1[num_train:],\n",
    "                               test=drrn_phase2)\n",
    "        dset = TensorDict(rrn=rrn_dset, drrn=drrn_dset)\n",
    "        datasets.append(dset)\n",
    "        all_conditions.append(conditions)\n",
    "        \n",
    "    dataset = TensorDict.stack(datasets, 0)\n",
    "    dataset.save(save_path + 'hs_data.td')\n",
    "    df_conditions = pd.concat(all_conditions)\n",
    "    df_conditions.to_csv(save_path + 'hs_conditions.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_kwargs = pu.crossing(run_id=range(10),\n",
    "                        num_train=[25, 50, 100, 300, 500],\n",
    "                        model_type=['drrn', 'rrn'])\n",
    "mp = GPUMultiprocessor(run, df_kwargs, devices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "mp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_results = []\n",
    "te_results = []\n",
    "for filename in tqdm(glob.glob('/data2/pdp/ajhnam/hidden_singles_public/*_hs/**/tr_results.tsv')):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    tr_results.append(df)\n",
    "for filename in tqdm(glob.glob('/data2/pdp/ajhnam/hidden_singles_public/*_hs/**/te_results.tsv')):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    te_results.append(df)\n",
    "tr_results = pd.concat(tr_results)\n",
    "te_results = pd.concat(te_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_results.to_csv('/data2/pdp/ajhnam/hidden_singles_public/rrn_hs_tr_results2.tsv', sep='\\t', index=False)\n",
    "te_results.to_csv('/data2/pdp/ajhnam/hidden_singles_public/rrn_hs_te_results2.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
